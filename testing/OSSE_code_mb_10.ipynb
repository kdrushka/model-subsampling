{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456083db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Native packages\n",
    "from math import radians, degrees, sin, cos, asin, acos, sqrt\n",
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Third-party packages for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# Third-party packages for data interpolation\n",
    "\n",
    "from scipy import interpolate\n",
    "from xgcm import Grid\n",
    "\n",
    "# Third-party packages for data visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "\n",
    "from netrc import netrc\n",
    "from urllib import request\n",
    "from platform import system\n",
    "from getpass import getpass\n",
    "from http.cookiejar import CookieJar\n",
    "from os.path import expanduser, join\n",
    "from datetime import datetime, date, time, timedelta\n",
    "import gsw as sw\n",
    "import numpy as np\n",
    "import xgcm.grid\n",
    "import netCDF4 as nc4\n",
    "\n",
    "\n",
    "#MB\n",
    "import matplotlib.dates as mdates\n",
    "import s3fs\n",
    "import numba\n",
    "from numba import jit\n",
    "\n",
    "# ***This library includes*** \n",
    "# - setup_earthdata_login_auth\n",
    "# - download_llc4320_data\n",
    "# - compute_derived_fields\n",
    "# - get_survey_track\n",
    "# - survey_interp\n",
    "# - great_circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad74619-e4e5-48ba-89de-93f08a396b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "from django.urls import path\n",
    "from fastai.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a24520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_earthdata_login_auth(endpoint: str='urs.earthdata.nasa.gov'):\n",
    "    netrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n",
    "    try:\n",
    "        username, _, password = netrc(file=join(expanduser('~'), netrc_name)).authenticators(endpoint)\n",
    "    except (FileNotFoundError, TypeError):\n",
    "        print('Please provide your Earthdata Login credentials for access.')\n",
    "        print('Your info will only be passed to %s and will not be exposed in Jupyter.' % (endpoint))\n",
    "        username = input('Username: ')\n",
    "        password = getpass('Password: ')\n",
    "    manager = request.HTTPPasswordMgrWithDefaultRealm()\n",
    "    manager.add_password(None, endpoint, username, password)\n",
    "    auth = request.HTTPBasicAuthHandler(manager)\n",
    "    jar = CookieJar()\n",
    "    processor = request.HTTPCookieProcessor(jar)\n",
    "    opener = request.build_opener(auth, processor)\n",
    "    request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b33476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_vector_to_EN(U, V, AngleCS, AngleSN):\n",
    "                \"\"\"\n",
    "                rotate vector to east north direction.\n",
    "                Assumes that AngleCS and AngleSN are already of same dimension as V and U (i.e. already interpolated to cell center)\n",
    "                Parameters\n",
    "                ----------\n",
    "                U: xarray Dataarray\n",
    "                    zonal vector component\n",
    "                V: xarray Dataarray\n",
    "                    meridional vector component\n",
    "                AngleCS: xarray Dataarray\n",
    "                    Cosine of angle of the grid center relative to the geographic direction\n",
    "                AngleSN: xarray Dataarray\n",
    "                    Sine of angle of the grid center relative to the geographic direction\n",
    "                Returns\n",
    "                ----------\n",
    "                uE: xarray Dataarray\n",
    "                    rotated zonal velocity\n",
    "                vN: xarray Dataarray\n",
    "                    rotated meridional velocity\n",
    "                    \n",
    "                    \n",
    "                adapted from https://github.com/AaronDavidSchneider/cubedsphere/blob/main/cubedsphere/regrid.py\n",
    "            \n",
    "                \"\"\"\n",
    "                # rotate the vectors:\n",
    "                uE = AngleCS * U - AngleSN * V\n",
    "                vN = AngleSN * U + AngleCS * V\n",
    "\n",
    "                return uE, vN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8e665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_llc4320_data(RegionName, datadir, start_date, ndays):\n",
    "    \"\"\"\n",
    "    Check for existing llc4320 files in 'datadir' and download if they aren't found\n",
    "    inputs XXX\n",
    "    \"\"\"\n",
    "    ShortName = \"MITgcm_LLC4320_Pre-SWOT_JPL_L4_\" + RegionName + \"_v1.0\"\n",
    "    date_list = [start_date + timedelta(days=x) for x in range(ndays)]\n",
    "    target_files = [f'LLC4320_pre-SWOT_{RegionName}_{date_list[n].strftime(\"%Y%m%d\")}.nc' for n in range(ndays)] # list of files to check for/download\n",
    "    setup_earthdata_login_auth()\n",
    "    \n",
    "    # https access for each target_file\n",
    "    url = \"https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected\"\n",
    "    https_accesses = [f\"{url}/{ShortName}/{target_file}\" for target_file in target_files]\n",
    "#     print(https_accesses)\n",
    "    \n",
    "\n",
    "    Path(datadir).mkdir(parents=True, exist_ok=True) # create datadir if it doesn't exist\n",
    "\n",
    "    # list of dataset objects\n",
    "    dds = []\n",
    "    for https_access,target_file in zip(https_accesses,target_files):\n",
    "        \n",
    "\n",
    "        if not(os.path.isfile(datadir + target_file)):\n",
    "            print('downloading ' + target_file) # print file name\n",
    "            try:\n",
    "                filename_dir = os.path.join(datadir, target_file)\n",
    "                request.urlretrieve(https_access, filename_dir)\n",
    "            except:\n",
    "                print(' ---- error - skipping this file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2d2181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checks(datadir):\n",
    "    \"\"\"\n",
    "    Check for derived files in {datadir}/derived and compute if the files don't exist\n",
    "    \n",
    "    *Add checks for user entries\n",
    "    \"\"\"\n",
    "    # directory to save derived data to - create if doesn't exist\n",
    "    derivedir = datadir + 'derived/'\n",
    "    if not(os.path.isdir(derivedir)):\n",
    "        os.mkdir(derivedir)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd9225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_derived_fields(RegionName, datadir, start_date, ndays):\n",
    "    \"\"\"\n",
    "    Check for derived files in {datadir}/derived and compute if the files don't exist\n",
    "    \"\"\"\n",
    "    # directory to save derived data to - create if doesn't exist\n",
    "    derivedir = datadir + 'derived/'\n",
    "    if not(os.path.isdir(derivedir)):\n",
    "        os.mkdir(derivedir)\n",
    "        \n",
    "    # files to load:\n",
    "    date_list = [start_date + timedelta(days=x) for x in range(ndays)]\n",
    "    target_files = [f'{datadir}LLC4320_pre-SWOT_{RegionName}_{date_list[n].strftime(\"%Y%m%d\")}.nc' for n in range(ndays)] # list target files\n",
    "    \n",
    "    # list of derived files:\n",
    "    derived_files = [f'{derivedir}LLC4320_pre-SWOT_{RegionName}_derived-fields_{date_list[n].strftime(\"%Y%m%d\")}.nc' for n in range(ndays)] # list target files\n",
    "\n",
    "        \n",
    "    # loop through input files, then do the following:\n",
    "    # - compute steric height\n",
    "    # - interpolate vector quantities (velocity and wind) to the tracer grid\n",
    "    # - rotate vectoor quantities to the geophysical (east/north) grid \n",
    "    # - compute vorticity (on the transformed grid)\n",
    "    fis = range(len(target_files))\n",
    "    \n",
    "    cnt = 0 # count\n",
    "    for fi in fis:\n",
    "        # input filename:\n",
    "        thisf=target_files[fi]\n",
    "        # output filename:\n",
    "        fnout = thisf.replace(RegionName + '_' , RegionName + '_derived-fields_')\n",
    "        fnout = fnout.replace(RegionName + '/' , RegionName + '/derived/')\n",
    "        # check if output file already exists\n",
    "        if (not(os.path.isfile(fnout))):   \n",
    "            print('computing derived fields for', thisf) \n",
    "            # load file:\n",
    "            ds = xr.open_dataset(thisf)\n",
    "            \n",
    "            # -------\n",
    "            # first time through the loop, load reference profile:\n",
    "            # load a single file to get coordinates\n",
    "            if cnt==0:\n",
    "                # mean lat/lon of domain\n",
    "                xav = ds.XC.isel(j=0).mean(dim='i')\n",
    "                yav = ds.YC.isel(i=0).mean(dim='j')\n",
    "\n",
    "                # for transforming U and V, and for the vorticity calculation, build the xgcm grid:\n",
    "                # see https://xgcm.readthedocs.io/en/latest/xgcm-examples/02_mitgcm.html\n",
    "                grid = xgcm.Grid(ds, coords={'X':{'center': 'i', 'left': 'i_g'}, \n",
    "                             'Y':{'center': 'j', 'left': 'j_g'},\n",
    "                             'T':{'center': 'time'},\n",
    "                             'Z':{'center': 'k'}})\n",
    "                \n",
    "\n",
    "                # --- load reference file of argo data\n",
    "                # here we use the 3x3 annual mean Argo product on standard produced by IRPC & distributed by ERDDAP\n",
    "                # https://apdrc.soest.hawaii.edu/erddap/griddap/hawaii_soest_defb_b79c_cb17.html\n",
    "                # - download the profile closest to xav,yav once (quick), use it, then delete it.\n",
    "                \n",
    "                # URL gets temp & salt at all levels\n",
    "                argofile = f'https://apdrc.soest.hawaii.edu/erddap/griddap/hawaii_soest_625d_3b64_cc4d.nc?temp[(0000-12-15T00:00:00Z):1:(0000-12-15T00:00:00Z)][(0.0):1:(2000.0)][({yav.data}):1:({yav.data})][({xav.data}):1:({xav.data})],salt[(0000-12-15T00:00:00Z):1:(0000-12-15T00:00:00Z)][(0.0):1:(2000.0)][({yav.data}):1:({yav.data})][({xav.data}):1:({xav.data})]'\n",
    "                \n",
    "                # delete the argo file if it exists \n",
    "                if os.path.isfile('argo_local.nc'):\n",
    "                    os.remove('argo_local.nc')\n",
    "                # use requests to get the file, and write locally:\n",
    "                r = requests.get(argofile)\n",
    "                file = open('argo_local.nc','wb')\n",
    "                file.write(r.content)\n",
    "                file.close()\n",
    "                # open the argo file:\n",
    "                argods = xr.open_dataset('argo_local.nc',decode_times=False)\n",
    "                # get rid of time coord/dim/variable, which screws up the time in ds if it's loaded\n",
    "                argods = argods.squeeze().reset_coords(names = {'time'}, drop=True) \n",
    "                # reference profiles: annual average Argo T/S using nearest neighbor\n",
    "                Tref = argods[\"temp\"]\n",
    "                Sref = argods[\"salt\"]\n",
    "                # SA and CT from gsw:\n",
    "                # see example from https://discourse.pangeo.io/t/wrapped-for-dask-teos-10-gibbs-seawater-gsw-oceanographic-toolbox/466\n",
    "                Pref = xr.apply_ufunc(sw.p_from_z, -argods.LEV, yav)\n",
    "                Pref.compute()\n",
    "                SAref = xr.apply_ufunc(sw.SA_from_SP, Sref, Pref, xav, yav,\n",
    "                                       dask='parallelized', output_dtypes=[Sref.dtype])\n",
    "                SAref.compute()\n",
    "                CTref = xr.apply_ufunc(sw.CT_from_pt, Sref, Tref, # Theta is potential temperature\n",
    "                                       dask='parallelized', output_dtypes=[Sref.dtype])\n",
    "                CTref.compute()\n",
    "                Dref = xr.apply_ufunc(sw.density.rho, SAref, CTref, Pref,\n",
    "                                    dask='parallelized', output_dtypes=[Sref.dtype])\n",
    "                Dref.compute()\n",
    "                \n",
    "                \n",
    "                cnt = cnt+1\n",
    "                print()\n",
    "                \n",
    "            # -------\n",
    "            \n",
    "            # --- COMPUTE STERIC HEIGHT IN STEPS ---\n",
    "            # 0. create datasets for variables of interest:\n",
    "            ss = ds.Salt\n",
    "            tt = ds.Theta\n",
    "            pp = xr.DataArray(sw.p_from_z(ds.Z,ds.YC))\n",
    "            \n",
    "            # 1. compute absolute salinity and conservative temperature\n",
    "            sa = xr.apply_ufunc(sw.SA_from_SP, ss, pp, xav, yav, dask='parallelized', output_dtypes=[ss.dtype])\n",
    "            sa.compute()\n",
    "            ct = xr.apply_ufunc(sw.CT_from_pt, sa, tt, dask='parallelized', output_dtypes=[ss.dtype])\n",
    "            ct.compute()\n",
    "            dd = xr.apply_ufunc(sw.density.rho, sa, ct, pp, dask='parallelized', output_dtypes=[ss.dtype])\n",
    "            dd.compute()\n",
    "            # 2. compute specific volume anomaly: gsw.density.specvol_anom_standard(SA, CT, p)\n",
    "            sva = xr.apply_ufunc(sw.density.specvol_anom_standard, sa, ct, pp, dask='parallelized', output_dtypes=[ss.dtype])\n",
    "            sva.compute()\n",
    "            # 3. compute steric height = integral(0:z1) of Dref(z)*sva(z)*dz(z)\n",
    "            # - first, interpolate Dref to the model pressure levels\n",
    "            Drefi = Dref.interp(LEV=-ds.Z)\n",
    "            dz = -ds.Z_bnds.diff(dim='nb').drop_vars('nb').squeeze() # distance between interfaces\n",
    "\n",
    "            # steric height computation (summation/integral)\n",
    "            # - increase the size of Drefi and dz to match the size of sva\n",
    "            Db = Drefi.broadcast_like(sva)\n",
    "            dzb = dz.broadcast_like(sva)\n",
    "            dum = Db * sva * dzb\n",
    "            sh = dum.cumsum(dim='k') \n",
    "            # this gives sh as a 3-d variable, (where the depth dimension \n",
    "            # represents the deepest level from which the specific volume anomaly was interpolated)\n",
    "            # - but in reality we just want the SH that was determined by integrating over\n",
    "            # the full survey depth, which gives a 2-d output:\n",
    "            sh_true = dum.sum(dim='k') \n",
    "            \n",
    "            # --- COMPUTE VORTICITY using xgcm and interpolate to X, Y\n",
    "            # see https://xgcm.readthedocs.io/en/latest/xgcm-examples/02_mitgcm.html\n",
    "            vorticity = (grid.diff(ds.V*ds.DXG, 'X') - grid.diff(ds.U*ds.DYG, 'Y'))/ds.RAZ\n",
    "            vorticity = grid.interp(grid.interp(vorticity, 'X', boundary='extend'), 'Y', boundary='extend')\n",
    "            \n",
    "            \n",
    "\n",
    "            # --- ROTATE AND TRANSFORM VECTOR QUANTITIES ---\n",
    "            # interpolate U,V and oceTAUX, oceTAUY to the tracer grid\n",
    "            # and rotate them to geophysical (east, north) coordinates instead of model ones:\n",
    "            # 1) regrid \n",
    "            print('interpolating to tracer grid')\n",
    "            U_c = grid.interp(ds.U, 'X', boundary='extend')\n",
    "            V_c = grid.interp(ds.V, 'Y', boundary='extend')\n",
    "            # do the same for TAUX and TAUY:\n",
    "            oceTAUX_c = grid.interp(ds.oceTAUX, 'X', boundary='extend')\n",
    "            oceTAUY_c = grid.interp(ds.oceTAUY, 'Y', boundary='extend')\n",
    "\n",
    "            # 2) rotate U and V, and taux and tauy, using rotate_vector_to_EN:\n",
    "            print('rotating to east/north')\n",
    "            U_transformed, V_transformed = rotate_vector_to_EN(U_c, V_c, ds['AngleCS'], ds['AngleSN'])\n",
    "            oceTAUX_transformed, oceTAUY_transformed = rotate_vector_to_EN(oceTAUX_c, oceTAUY_c, ds['AngleCS'], ds['AngleSN'])\n",
    "\n",
    "            # --- save derived fields in a new file\n",
    "            # - convert sh and zeta to datasets\n",
    "            # NOTE can do this more efficiently in a single line w/out converting to dataset???\n",
    "            dout = vorticity.to_dataset(name='vorticity')\n",
    "            sh_ds = sh.to_dataset(name='steric_height')\n",
    "            dout = dout.merge(sh_ds)\n",
    "            sh_true_ds = sh_true.to_dataset(name='steric_height_true')\n",
    "            dout = dout.merge(sh_true_ds)\n",
    "            U_transformed_ds = U_transformed.to_dataset(name='U_transformed')\n",
    "            V_transformed_ds = V_transformed.to_dataset(name='V_transformed')\n",
    "            oceTAUX_transformed_ds = oceTAUX_transformed.to_dataset(name='oceTAUX_transformed')\n",
    "            oceTAUY_transformed_ds = oceTAUY_transformed.to_dataset(name='oceTAUY_transformed')\n",
    "            dout = dout.merge(U_transformed_ds).merge(V_transformed_ds)\n",
    "            dout = dout.merge(oceTAUX_transformed_ds).merge(oceTAUY_transformed_ds)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # add/rename the Argo reference profile variables to dout:\n",
    "            tref = Tref.to_dataset(name='Tref')\n",
    "            tref = tref.merge(Sref).rename({'salt': 'Sref'}).\\\n",
    "                rename({'LEV':'zref','latitude':'yav','longitude':'xav'})\n",
    "            # - add ref profiles to dout and drop uneeded vars/coords\n",
    "            dout = dout.merge(tref).drop_vars({'longitude','latitude','LEV'})\n",
    "  \n",
    "    \n",
    "            # - add attributes for all variables\n",
    "            dout.steric_height.attrs = {'long_name' : 'Steric height',\n",
    "                                    'units' : 'm',\n",
    "                                    'comments_1' : 'Computed by integrating the specific volume anomaly (SVA) multiplied by a reference density, where the reference density profile is calculated from temperature & salinity profiles from the APDRC 3x3deg gridded Argo climatology product (accessed through ERDDAP). The profile nearest to the center of the domain is selected, and T & S profiles are averaged over one year before computing ref density. SVA is computed from the model T & S profiles. the Gibbs Seawater Toolbox is used compute reference density and SVA. steric_height is given at all depth levels (dep): steric_height at a given depth represents steric height signal generated by the water column above that depth - so the deepest steric_height value represents total steric height (and is saved in steric_height_true'\n",
    "                                       }\n",
    "            dout.steric_height_true.attrs = dout.steric_height.attrs\n",
    "            \n",
    "            dout.vorticity.attrs = {'long_name' : 'Vertical component of the vorticity',\n",
    "                                    'units' : 's-1',\n",
    "                                    'comments_1' : 'computed on DXG,DYG then interpolated to X,Y'}\n",
    "            \n",
    "            dout.U_transformed.attrs['long_name'] = \"Horizontal velocity in the eastward direction\"\n",
    "            dout.U_transformed.attrs['comments_1'] = \"Horizontal velocity in the eastward direction at the center of the tracer cell on the native model grid.\"\n",
    "            dout.U_transformed.attrs['comments_3'] = \"Note: this has been transformed to the tracer grid and rotated to geophysical coordinates.\"\n",
    "\n",
    "            dout.V_transformed.attrs['long_name'] = \"Horizontal velocity in the northward direction\"\n",
    "            dout.V_transformed.attrs['comments_1'] = \"Horizontal velocity in the northward direction at the center of the tracer cell on the native model grid.\"\n",
    "            dout.V_transformed.attrs['comments_3'] = \"Note: this has been transformed to the tracer grid and rotated to geophysical coordinates.\"\n",
    "\n",
    "            dout.oceTAUX_transformed.attrs['long_name'] = \"Ocean surface stress in the eastward direction\"\n",
    "            dout.oceTAUX_transformed.attrs['comments_1'] = \"Ocean surface stress due to wind and sea-ice in the eastward direction centered over the the native model grid\"\n",
    "            dout.oceTAUX_transformed.attrs['comments_3'] = \"Note: this has been transformed to the tracer grid and rotated to geophysical coordinates.\"\n",
    "\n",
    "            dout.oceTAUY_transformed.attrs['long_name'] = \"Ocean surface stress in the northward direction\"\n",
    "            dout.oceTAUY_transformed.attrs['comments_1'] = \"Ocean surface stress due to wind and sea-ice in the northward direction centered over the the native model grid\"\n",
    "            dout.oceTAUY_transformed.attrs['comments_3'] = \"Note: this has been transformed to the tracer grid and rotated to geophysical coordinates.\"\n",
    "            \n",
    "            \n",
    "            \n",
    "            dout.Tref.attrs = {'long_name' : f'Reference temperature profile at {yav.data}N,{xav.data}E',\n",
    "                                    'units' : 'degree_C',\n",
    "                                    'comments_1' : 'From Argo 3x3 climatology produced by APDRC'}\n",
    "            dout.Sref.attrs = {'long_name' : f'Reference salinity profile at {yav.data}N,{xav.data}E',\n",
    "                                    'units' : 'psu',\n",
    "                                    'comments_1' : 'From Argo 3x3 climatology produced by APDRC'}\n",
    "            \n",
    "            dout.zref.attrs = {'long_name' : f'Reference depth for Tref and Sref',\n",
    "                                    'units' : 'm',\n",
    "                                    'comments_1' : 'From Argo 3x3 climatology produced by APDRC'}\n",
    "            \n",
    "            \n",
    "            # - save netcdf file with derived fields\n",
    "            netcdf_fill_value = nc4.default_fillvals['f4']\n",
    "            dv_encoding = {}\n",
    "            for dv in dout.data_vars:\n",
    "                dv_encoding[dv]={'zlib':True,  # turns compression on\\\n",
    "                            'complevel':9,     # 1 = fastest, lowest compression; 9=slowest, highest compression \\\n",
    "                            'shuffle':True,    # shuffle filter can significantly improve compression ratios, and is on by default \\\n",
    "                            'dtype':'float32',\\\n",
    "                            '_FillValue':netcdf_fill_value}\n",
    "            # save to a new file\n",
    "            print(' ... saving to ', fnout)\n",
    "            # TROUBLESHOOTING::::: DELETE THE RETURN LINE\n",
    "            #return dout, dv_encoding\n",
    "            dout.to_netcdf(fnout,format='netcdf4',encoding=dv_encoding)\n",
    "\n",
    "            \n",
    "            \n",
    "            # release & delete Argo file\n",
    "            argods.close()\n",
    "#             os.remove('argo_local.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4efd30-a1ba-4af4-a3f7-d10950864094",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_survey_track(ds, sampling_details):\n",
    "     \n",
    "    \"\"\"\n",
    "    Returns the track (lat, lon, depth, time) and indices (i, j, k, time) of the \n",
    "    sampling trajectory based on the type of sampling (sampling_details[SAMPLING_STRATEGY]), \n",
    "    and sampling details (in dict sampling_details), which includes\n",
    "    number of days, waypoints, and depth range, horizontal and vertical platform speed\n",
    "    -- these can be typical values (default) or user-specified (optional)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    survey_time_total = (ds.time.values.max() - ds.time.values.min()) # (timedelta) - limits the survey to a total time\n",
    "    survey_end_time = ds.time.isel(time=0).data + survey_time_total # end time of survey\n",
    "    # Convert lon, lat and z to index i, j and k with f_x, f_y and f_z\n",
    "    # XC, YC and Z are the same at all times, so select a single time\n",
    "    X = ds.XC.isel(time=0) \n",
    "    Y = ds.YC.isel(time=0)\n",
    "    i = ds.i\n",
    "    j = ds.j\n",
    "    z = ds.Z.isel(time=0)\n",
    "    k = ds.k\n",
    "    f_x = interpolate.interp1d(X[0,:].values, i)\n",
    "    f_y = interpolate.interp1d(Y[:,0].values, j)\n",
    "    f_z = interpolate.interp1d(z, k, bounds_error=False)\n",
    "\n",
    "    # Get boundaries and center of model region\n",
    "    model_boundary_n = Y.max().values\n",
    "    model_boundary_s = Y.min().values\n",
    "    model_boundary_w = X.min().values\n",
    "    model_boundary_e = X.max().values\n",
    "    model_xav = ds.XC.isel(time=0, j=0).mean(dim='i').values\n",
    "    model_yav = ds.YC.isel(time=0, i=0).mean(dim='j').values\n",
    "    # --------- define sampling -------\n",
    "    SAMPLING_STRATEGY = sampling_details['SAMPLING_STRATEGY']\n",
    "    # ------ default sampling parameters: in the dict named \"defaults\" -----\n",
    "    defaults = {}\n",
    "    # default values depend on the sampling type\n",
    "    # typical speeds and depth ranges based on platform \n",
    "    if SAMPLING_STRATEGY == 'sim_uctd':\n",
    "        # typical values for uctd sampling:\n",
    "        defaults['zrange'] = [-5, -500] # depth range of profiles (down is negative)\n",
    "        defaults['hspeed'] = 5 # platform horizontal speed in m/s\n",
    "        defaults['vspeed'] = 1 # platform vertical (profile) speed in m/s (NOTE: may want different up/down speeds)  \n",
    "        defaults['PATTERN'] = 'lawnmower'\n",
    "        defaults['AT_END'] = 'terminate'  # behaviour at and of trajectory: 'repeat', 'reverse', or 'terminate'\n",
    "    elif SAMPLING_STRATEGY == 'sim_glider':\n",
    "        defaults['zrange'] = [-1, -1000] # depth range of profiles (down is negative)\n",
    "        defaults['hspeed'] = 0.25 # platform horizontal speed in m/s\n",
    "        defaults['vspeed'] = 0.1 # platform vertical (profile) speed in m/s     \n",
    "        defaults['AT_END'] = 'terminate'  # behaviour at and of trajectory: 'repeat', 'reverse', or 'terminate'\n",
    "        defaults['PATTERN'] = 'lawnmower'\n",
    "        #MB\n",
    "    elif SAMPLING_STRATEGY == 'wave_glider':\n",
    "        defaults['zrange'] = [-6, -100] # depth range of profiles (down is negative)\n",
    "        defaults['hspeed'] = 1 # platform horizontal speed in m/s\n",
    "        defaults['vspeed'] = 0 # platform vertical (profile) speed in m/s     \n",
    "        defaults['AT_END'] = 'terminate'  # behaviour at and of trajectory: 'repeat', 'reverse', or 'terminate'\n",
    "        defaults['PATTERN'] = 'back-forth'\n",
    "        #MB\n",
    "    elif SAMPLING_STRATEGY == 'sail_drone':\n",
    "        defaults['zrange'] = [-1, -3] # depth range of profiles (down is negative)\n",
    "        defaults['hspeed'] = 2.57 # platform horizontal speed in m/s\n",
    "        defaults['vspeed'] = 0 # platform vertical (profile) speed in m/s     \n",
    "        defaults['AT_END'] = 'terminate'  # behaviour at and of trajectory: 'repeat', 'reverse', or 'terminate'\n",
    "        defaults['PATTERN'] = 'back-forth'\n",
    "        #MB\n",
    "    elif SAMPLING_STRATEGY == 'sim_mooring' or SAMPLING_STRATEGY == 'mooring':\n",
    "        defaults['xmooring'] = model_xav # default lat/lon is the center of the domain\n",
    "        defaults['ymooring'] = model_yav\n",
    "        defaults['zmooring_TS'] = [-1, -10, -50, -100] # depth of T/S instruments\n",
    "        defaults['zmooring_UV'] = [-1, -10, -50, -100] # depth of U/V instruments\n",
    "    elif SAMPLING_STRATEGY == 'trajectory_file':\n",
    "        # load file\n",
    "        traj = xr.open_dataset(sampling_details['trajectory_file'])\n",
    "        defaults['xwaypoints'] = traj.xwaypoints.values\n",
    "        defaults['ywaypoints'] = traj.ywaypoints.values\n",
    "        defaults['zrange'] = traj.zrange.values # depth range of profiles (down is negative)\n",
    "        defaults['hspeed'] = traj.hspeed.values # platform horizontal speed in m/s\n",
    "        defaults['vspeed'] = traj.vspeed.values # platform vertical (profile) speed in m/s\n",
    "        defaults['PATTERN'] = traj.attrs['pattern']\n",
    "    else:\n",
    "        # if SAMPLING_STRATEGY not specified, return an error\n",
    "        print('error: SAMPLING_STRATEGY ' + SAMPLING_STRATEGY + ' invalid')\n",
    "        return -1\n",
    "    \n",
    "    #\n",
    "    defaults['SAVE_PRELIMINARY'] = False\n",
    "    \n",
    "    \n",
    "    # merge defaults & sampling_details\n",
    "    # - by putting sampling_details second, items that appear in both dicts are taken from sampling_details: \n",
    "    sampling_details = {**defaults, **sampling_details}\n",
    "\n",
    "    # ----- define x/y/z/t points to interpolate to\n",
    "    # for moorings, location is fixed so a set of waypoints is not needed.\n",
    "    # however, for \"sim_mooring\", tile/repeat the sampling x/y/t to form 2-d arrays,\n",
    "    # so the glider/uCTD interpolation framework can be used.\n",
    "    # - and for \"mooring\", skip the step of interpolating to \"points\" and interpolate directly to the new x/y/t/z \n",
    "    if SAMPLING_STRATEGY == 'sim_mooring':\n",
    "        # time sampling is one per model timestep\n",
    "#         ts = ds.time.values / 24 # convert from hours to days\n",
    "        ts = ds.time.values # in hours\n",
    "        n_samples = ts.size\n",
    "        n_profiles = n_samples\n",
    "        # same sampling for T/S/U/V for now. NOTE: change this later!        \n",
    "        zs = np.tile(sampling_details['zmooring_TS'], int(n_samples)) # sample depths * # of samples \n",
    "        xs = sampling_details['xmooring'] * np.ones(np.size(zs))  # all samples @ same x location\n",
    "        ys = sampling_details['ymooring'] * np.ones(np.size(zs))  # all samples @ same y location\n",
    "        ts = np.repeat(ts, len(sampling_details['zmooring_TS']))  # tile to match size of other fields. use REPEAT, not TILE to get the interpolation right.\n",
    "\n",
    "#         # depth sampling - different for TS and UV\n",
    "#         zs_TS = np.tile(zmooring_TS, int(n_samples))\n",
    "#         zs_UV = np.tile(zmooring_UV, int(n_samples))\n",
    "#         xs_TS = xmooring * np.ones(np.size(zs_TS))\n",
    "#         xs_UV = xmooring * np.ones(np.size(zs_UV))\n",
    "#         ys_TS = ymooring * np.ones(np.size(zs_TS))\n",
    "#         ys_UV = ymooring * np.ones(np.size(zs_UV))\n",
    "#         ts_TS = np.tile(ts, int(n_samples))\n",
    "        \n",
    "        \n",
    "#         lon_TS = xr.DataArray(xs_TS,dims='points'),\n",
    "#         lat_TS = xr.DataArray(ys_TS,dims='points'),\n",
    "#         dep_TS = xr.DataArray(zs_TS,dims='points'),\n",
    "#         time_TS = xr.DataArray(ts,dims='points')\n",
    "               \n",
    "#         lon = lon_TS\n",
    "#         lat = lat_TS\n",
    "#         dep = dep_TS\n",
    "#         time = time_TS\n",
    "    elif SAMPLING_STRATEGY == 'mooring':\n",
    "        ts = ds.time.values # in hours\n",
    "        # same sampling for T/S/U/V for now. NOTE: change this later!  \n",
    "        zs = sampling_details['zmooring_TS'] \n",
    "        xs = sampling_details['xmooring']\n",
    "        ys = sampling_details['ymooring']\n",
    "    else:\n",
    "        # --- if not a mooring, define waypoints  \n",
    "    \n",
    "        # define x & y waypoints and z range\n",
    "        # xwaypoints & ywaypoints must have the same size\n",
    "        if sampling_details['PATTERN'] == 'lawnmower':\n",
    "            # \"mow the lawn\" pattern - define all waypoints\n",
    "            if not(SAMPLING_STRATEGY == 'trajectory_file'):\n",
    "                # generalize the survey for this region\n",
    "                xwaypoints = model_boundary_w + 1 + [0, 0, 0.5, 0.5, 1, 1, 1.5, 1.5, 2, 2]\n",
    "                ywaypoints = model_boundary_s + [1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2]\n",
    "        elif sampling_details['PATTERN'] == 'back-forth':\n",
    "            if not(SAMPLING_STRATEGY == 'trajectory_file'):\n",
    "                # repeated back & forth transects - define the end-points\n",
    "                xwaypoints = model_xav + [-1, 1]\n",
    "                ywaypoints = model_yav + [-1, 1]\n",
    "            # repeat waypoints based on total # of transects: \n",
    "            dkm_per_transect = great_circle(xwaypoints[0], ywaypoints[0], xwaypoints[1], ywaypoints[1]) # distance of one transect in km\n",
    "#           # time per transect, seconds, as a np.timedelta64 value\n",
    "            t_per_transect = np.timedelta64(int(dkm_per_transect * 1000 / sampling_details['hspeed']), 's')    \n",
    "            num_transects = np.round(survey_time_total / t_per_transect)\n",
    "            for n in np.arange(num_transects):\n",
    "                xwaypoints = np.append(xwaypoints, xwaypoints[-2])\n",
    "                ywaypoints = np.append(ywaypoints, ywaypoints[-2])\n",
    "        if SAMPLING_STRATEGY == 'trajectory_file':\n",
    "            xwaypoints = sampling_details['xwaypoints']\n",
    "            ywaypoints = sampling_details['ywaypoints']\n",
    "        # if the survey pattern repeats, add the first waypoint to the end of the list of waypoints:\n",
    "        if sampling_details['AT_END'] == 'repeat': \n",
    "            xwaypoints = np.append(xwaypoints, xwaypoints[0])\n",
    "            ywaypoints = np.append(ywaypoints, ywaypoints[0])                \n",
    "        \n",
    "## Different function\n",
    "        # vertical resolution\n",
    "        # for now, use a constant  vertical resolution (NOTE: could make this a variable)\n",
    "        #if ((SAMPLING_STRATEGY != 'wave_glider') and (SAMPLING_STRATEGY != 'sail_drone')): \n",
    "        zresolution = 1 # meters\n",
    "        # max depth can't be deeper than the max model depth in this region\n",
    "        sampling_details['zrange'][1] = -np.min([-sampling_details['zrange'][1], ds.Depth.isel(time=1).max(...).values])        \n",
    "        zprofile = np.arange(sampling_details['zrange'][0],sampling_details['zrange'][1],-zresolution) # depths for one profile\n",
    "        ztwoway = np.append(zprofile,zprofile[-1::-1])\n",
    "        # time resolution of sampling (dt):\n",
    "        dt = zresolution / sampling_details['vspeed'] # sampling resolution in seconds\n",
    "        dt_td64 = np.timedelta64(int(dt), 's') # np.timedelta64 format\n",
    "        # for each timestep dt \n",
    "        deltah = sampling_details['hspeed']*dt # horizontal distance traveled per sample\n",
    "        deltav = sampling_details['vspeed']*dt # vertical distance traveled per sample\n",
    "\n",
    "        # determine the sampling locations in 2-d space\n",
    "        # - initialize sample locations xs, ys, zs, ts\n",
    "        xs = []\n",
    "        ys = []\n",
    "        zs = []\n",
    "        ts = []\n",
    "        dkm_total = 0 \n",
    "    \n",
    "\n",
    "        for w in np.arange(len(xwaypoints)-1):\n",
    "            # interpolate between this and the following waypoint:\n",
    "            dkm = great_circle(xwaypoints[w], ywaypoints[w], xwaypoints[w+1], ywaypoints[w+1])\n",
    "            # number of time steps (vertical measurements) between this and the next waypoint\n",
    "            nstep = int(dkm*1000 / deltah) \n",
    "            yi = np.linspace(ywaypoints[w], ywaypoints[w+1], nstep)\n",
    "            xi = np.linspace(xwaypoints[w], xwaypoints[w+1], nstep)\n",
    "            xi = xi[0:-1] # remove last point, which is the next waypoint\n",
    "            xs = np.append(xs, xi) # append\n",
    "            yi = yi[0:-1] # remove last point, which is the next waypoint\n",
    "            ys = np.append(ys, yi) # append\n",
    "            dkm_total = dkm_total + dkm           \n",
    "            # cumulative survey time to this point, in seconds, as a np.timedelta64 value\n",
    "            t_total = np.timedelta64(int(dkm_total * 1000 / sampling_details['hspeed']), 's')\n",
    "            \n",
    "            # cut off the survey after survey_time_total\n",
    "            if t_total > survey_time_total:\n",
    "                break \n",
    "                \n",
    "        # km for one lap of the survey\n",
    "        dkm_once = dkm_total \n",
    "\n",
    "        # if time is less than survey_time_total, trigger AT_END behavior:\n",
    "        if t_total < survey_time_total:\n",
    "            if sampling_details['AT_END'] == 'repeat': \n",
    "                # start at the beginning again\n",
    "                # - determine how many times the survey repeats:\n",
    "                num_transects = np.round(survey_time_total / t_total)\n",
    "                x_once = xs\n",
    "                y_once = ys\n",
    "                for n in np.arange(num_transects):\n",
    "                    xs = np.append(xs, x_once)\n",
    "                    ys = np.append(ys, y_once)\n",
    "                    dkm_total += dkm_once\n",
    "            elif sampling_details['AT_END'] == 'reverse': \n",
    "                # turn around & go in the opposite direction\n",
    "                # - determine how many times the survey repeats:\n",
    "                num_transects = np.round(survey_time_total / t_total)\n",
    "                x_once = xs\n",
    "                y_once = ys\n",
    "                # append both a backward & another forward transect\n",
    "                for n in np.arange(np.ceil(num_transects/2)):\n",
    "                    xs = np.append(np.append(xs, x_once[-2:1:-1]), x_once)\n",
    "                    ys = np.append(np.append(ys, y_once[-2:1:-1]), y_once)\n",
    "                    dkm_total += dkm_once*2\n",
    "\n",
    "\n",
    "        # repeat (tile) the two-way sampling depths \n",
    "        # - number of profiles we make during the survey:\n",
    "        n_profiles = np.ceil(xs.size / ztwoway.size)\n",
    "        zs = np.tile(ztwoway, int(n_profiles))\n",
    "        zs = zs[0:xs.size] # limit to # of sample times\n",
    "        ts = ds.time.isel(time=0).data + dt_td64 * np.arange(xs.size)\n",
    "        # get rid of points with sample time > survey_time_total\n",
    "        if survey_time_total > 0:\n",
    "            idx = np.argmin(np.abs(ts - survey_end_time))# index of ts closest to survey_end_time\n",
    "            print('originally, ', idx, ' points')\n",
    "            # make sure this is multiple of the # of profiles:\n",
    "            idx = int(np.floor((idx+1)/len(ztwoway)) * (len(ztwoway)))\n",
    "            xs = xs[:idx]\n",
    "            ys = ys[:idx]\n",
    "            ts = ts[:idx]\n",
    "            zs = zs[:idx]\n",
    "            n_profiles = np.ceil(xs.size / ztwoway.size)\n",
    "            # update t_total\n",
    "            t_total = np.diff(ts[[0,-1]])\n",
    "            t_total_seconds = int(t_total)/1e9 # convert from nanoseconds to seconds\n",
    "            # use the speed to determine dkm_total (time * hspeed)\n",
    "            dkm_total = t_total_seconds * sampling_details['hspeed'] / 1000\n",
    "            print('limited to ', idx, 'points: n_profiles=', n_profiles, ', ', len(zprofile), 'depths per profile, ', len(ztwoway), 'depths per two-way')\n",
    "            \n",
    "        sampling_details['distance_total_km'] = dkm_total\n",
    "        sampling_details['time_total_s'] = t_total_seconds  \n",
    "        # -- end if not a mooring\n",
    "        \n",
    "    # ----- Assemble dataset: -----\n",
    "    # (same regardless of sampling strategy - EXCEPT \"mooring\")\n",
    "    if not SAMPLING_STRATEGY == 'mooring':\n",
    "        # - real (lat/lon) coordinates:\n",
    "        survey_track = xr.Dataset(\n",
    "            dict(\n",
    "                lon = xr.DataArray(xs,dims='points'),\n",
    "                lat = xr.DataArray(ys,dims='points'),\n",
    "                dep = xr.DataArray(zs,dims='points'),\n",
    "                time = xr.DataArray(ts,dims='points'),\n",
    "                n_profiles = n_profiles\n",
    "            )\n",
    "        )\n",
    "        # - transform to i,j,k coordinates:\n",
    "        survey_indices= xr.Dataset(\n",
    "            dict(\n",
    "                i = xr.DataArray(f_x(survey_track.lon), dims='points'),\n",
    "                j = xr.DataArray(f_y(survey_track.lat), dims='points'),\n",
    "                k = xr.DataArray(f_z(survey_track.dep), dims='points'),\n",
    "                time = xr.DataArray(survey_track.time, dims='points'),\n",
    "            )\n",
    "        )\n",
    "    elif SAMPLING_STRATEGY == 'mooring':\n",
    "        survey_track = xr.Dataset(\n",
    "            dict(\n",
    "                lon = xr.DataArray(xs*[1], dims='position'),\n",
    "                lat = xr.DataArray(ys*[1], dims='position'),\n",
    "                dep = xr.DataArray(zs, dims='depth'),\n",
    "                time = xr.DataArray(ts, dims='time')\n",
    "\n",
    "            )\n",
    "        )\n",
    "        # - transform to i,j,k coordinates:\n",
    "        survey_indices= xr.Dataset(\n",
    "            dict(\n",
    "                i = xr.DataArray(f_x(survey_track.lon), dims='position'),\n",
    "                j = xr.DataArray(f_y(survey_track.lat), dims='position'),\n",
    "                k = xr.DataArray(f_z(survey_track.dep), dims='depth'),\n",
    "                time = xr.DataArray(survey_track.time, dims='time'),\n",
    "            )\n",
    "        )\n",
    "    # store SAMPLING_STRATEGY and DERIVED_VARIABLES in survey_track so they can be used later\n",
    "    survey_track['SAMPLING_STRATEGY'] = SAMPLING_STRATEGY\n",
    "#     survey_track['DERIVED_VARIABLES'] = sampling_details['DERIVED_VARIABLES']\n",
    "#    survey_track['SAVE_PRELIMINARY'] = sampling_details['SAVE_PRELIMINARY']\n",
    "    return survey_track, survey_indices, sampling_details\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9b4666",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def survey_interp(ds, survey_track, survey_indices, sampling_details):\n",
    "    \"\"\"\n",
    "    interpolate dataset 'ds' along the survey track given by \n",
    "    'survey_indices' (i,j,k coordinates used for the interpolation), and\n",
    "    'survey_track' (lat,lon,dep,time of the survey)\n",
    "    \n",
    "    Returns:\n",
    "        subsampled_data: all field interpolated onto the track\n",
    "        sh_true: 'true' steric height along the track\n",
    "    \n",
    "    \"\"\"\n",
    "      \n",
    "        \n",
    "    ## Create a new dataset to contain the interpolated data, and interpolate\n",
    "    # for 'mooring', skip this step entirely - return an empty array for 'subsampled_data'\n",
    "    SAMPLING_STRATEGY = survey_track['SAMPLING_STRATEGY']\n",
    "    if SAMPLING_STRATEGY == 'mooring':\n",
    "        subsampled_data = []\n",
    "        \n",
    "        # zgridded and times are simply zs, ta (i.e., don't interpolate to a finer grid than the mooring sampling gives)\n",
    "        zgridded = survey_track['dep']\n",
    "        times = survey_track['time']\n",
    "        \n",
    "        # -- initialize the dataset:\n",
    "        sgridded = xr.Dataset(\n",
    "            coords = dict(depth=([\"depth\"],zgridded),\n",
    "                      time=([\"time\"],times))\n",
    "        )\n",
    "        # variable names (if DERIVED_VARIABLES is not set, don't load the vector quantities)\n",
    "        if sampling_details['DERIVED_VARIABLES']:\n",
    "            vbls3d = ['Theta','Salt','vorticity','steric_height', 'U', 'V']\n",
    "            vbls2d = ['steric_height_true', 'Eta', 'KPPhbl', 'PhiBot', 'oceTAUX', 'oceTAUY', 'oceFWflx', 'oceQnet', 'oceQsw', 'oceSflux']\n",
    "        else:\n",
    "            vbls3d = ['Theta','Salt']\n",
    "            vbls2d = ['Eta', 'KPPhbl', 'PhiBot', 'oceFWflx', 'oceQnet', 'oceQsw', 'oceSflux']\n",
    "        \n",
    "        \n",
    "        # loop through 3d variables & interpolate:\n",
    "        for vbl in vbls3d:\n",
    "            print(vbl)\n",
    "            sgridded[vbl]=ds[vbl].interp(survey_indices).compute().transpose()\n",
    "\n",
    "        # loop through 2d variables & interpolate:\n",
    "        # create 2-d survey track by removing the depth dimension\n",
    "        survey_indices_2d =  survey_indices.drop_vars('k')\n",
    "           \n",
    "        \n",
    "        \n",
    "        for vbl in vbls2d:\n",
    "            print(vbl)\n",
    "            sgridded[vbl]=ds[vbl].interp(survey_indices_2d).compute()\n",
    "            \n",
    "        \n",
    "        # clean up sgridded: get rid of the dims we don't need and rename coords\n",
    "        \n",
    "        if sampling_details['DERIVED_VARIABLES']:\n",
    "            sgridded = sgridded.reset_coords(names = {'i', 'j', 'k'}).squeeze().rename_vars({'xav' : 'lon','yav' : 'lat'}).drop_vars(names={'i', 'j', 'k'})\n",
    "        \n",
    "            # for sampled steric height, we want the value integrated from the deepest sampling depth:\n",
    "            sgridded['steric_height'] = ((\"time\"), sgridded['steric_height'].isel(depth=int(len(zgridded))-1))\n",
    "            # rename to \"sampled\" for clarity\n",
    "            sgridded.rename_vars({'steric_height':'steric_height_sampled'})\n",
    "        else:\n",
    "            sgridded = sgridded.reset_coords(names = {'i', 'j', 'k'}).squeeze().drop_vars(names={'i', 'j', 'k'})\n",
    "    \n",
    "    else:\n",
    "        subsampled_data = xr.Dataset(\n",
    "            dict(\n",
    "                t = xr.DataArray(survey_track.time, dims='points'), # call this time, for now, so that the interpolation works\n",
    "                lon = xr.DataArray(survey_track.lon, dims='points'),\n",
    "                lat = xr.DataArray(survey_track.lat, dims='points'),\n",
    "                dep = xr.DataArray(survey_track.dep, dims='points'),\n",
    "                points = xr.DataArray(survey_track.points, dims='points')\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # variable names (if DERIVED_VARIABLES is not set, don't load the vector quantities)\n",
    "        if sampling_details['DERIVED_VARIABLES']:\n",
    "            vbls3d = ['Theta','Salt','vorticity','steric_height', 'U', 'V']\n",
    "            vbls2d = ['steric_height_true', 'Eta', 'KPPhbl', 'PhiBot', 'oceTAUX', 'oceTAUY', 'oceFWflx', 'oceQnet', 'oceQsw', 'oceSflux']\n",
    "        else:\n",
    "            vbls3d = ['Theta','Salt']\n",
    "            vbls2d = ['Eta', 'KPPhbl', 'PhiBot', 'oceFWflx', 'oceQnet', 'oceQsw', 'oceSflux']\n",
    "        \n",
    "        \n",
    "        print('Interpolating model fields to the sampling track...')\n",
    "        # loop & interpolate through 3d variables:\n",
    "        for vbl in vbls3d:\n",
    "            subsampled_data[vbl]=ds[vbl].interp(survey_indices)\n",
    "\n",
    "       \n",
    "\n",
    "        # loop & interpolate through 2d variables:\n",
    "        # create 2-d survey track by removing the depth dimension\n",
    "        survey_indices_2d =  survey_indices.drop_vars('k')\n",
    "        for vbl in vbls2d:\n",
    "            subsampled_data[vbl]=ds[vbl].interp(survey_indices_2d)   \n",
    "\n",
    "        # fix time, which is currently a coordinate (time) & a variable (t)\n",
    "        subsampled_data = subsampled_data.reset_coords('time', drop=True).rename_vars({'t':'time'})\n",
    "\n",
    "        # make xav and yav variables instead of coords, and rename\n",
    "        if sampling_details['DERIVED_VARIABLES']:\n",
    "            subsampled_data = subsampled_data.reset_coords(names = {'xav','yav'}).rename_vars({'xav' : 'lon_average','yav' : 'lat_average'})\n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "        if sampling_details['SAVE_PRELIMINARY']:\n",
    "            # ----- save preliminary data\n",
    "            # (not applicable to mooring data)\n",
    "            # add metadata to attributes\n",
    "            attrs = sampling_details\n",
    "            attrs['start_date'] = sampling_details['start_date'].strftime('%Y-%m-%d')\n",
    "            end_date = subsampled_data['time'].data[-1]\n",
    "            attrs['end_date'] = np.datetime_as_string(end_date,unit='D')\n",
    "            attrs.pop('DERIVED_VARIABLES')    \n",
    "            attrs.pop('SAVE_PRELIMINARY')\n",
    "            \n",
    "            # filename:\n",
    "            filename_out = sampling_details['filename_out_base'] + '_subsampled.nc'\n",
    "            print(f'saving to {filename_out}')\n",
    "            subsampled_data.attrs = attrs\n",
    "            netcdf_fill_value = nc4.default_fillvals['f4']\n",
    "            dv_encoding={'zlib':True,  # turns compression on\\\n",
    "                        'complevel':9,     # 1 = fastest, lowest compression; 9=slowest, highest compression \\\n",
    "                        'shuffle':True,    # shuffle filter can significantly improve compression ratios, and is on by default \\\n",
    "                        'dtype':'float32',\\\n",
    "                        '_FillValue':netcdf_fill_value}\n",
    "            # save to a new file\n",
    "            subsampled_data.to_netcdf(filename_out,format='netcdf4')\n",
    "            \n",
    "            \n",
    "            \n",
    "        # -----------------------------------------------------------------------------------\n",
    "        # ------Regrid the data to depth/time (3-d fields) or subsample to time (2-d fields)\n",
    "        print('Gridding the interpolated data...')\n",
    "        \n",
    "        # if SAVE PRELIMINARY, load the saved 'subsampled_data' file without dask\n",
    "        # (otherwise, subsampled_data is already in memory)\n",
    "        if sampling_details['SAVE_PRELIMINARY']:\n",
    "            # now, reload with no chunking/dask and do the regridding\n",
    "            subsampled_data = xr.open_dataset(filename_out)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # get times associated with profiles:\n",
    "        if SAMPLING_STRATEGY == 'sim_mooring':\n",
    "            # - for mooring, use the subsampled time grid:\n",
    "            times = np.unique(subsampled_data.time.values)\n",
    "        else:\n",
    "            # -- for glider/uctd, take the shallowest & deepest profiles (every second value, since top/bottom get sampled twice for each profile)\n",
    "            time_deepest = subsampled_data.time.where(subsampled_data.dep == subsampled_data.dep.min(), drop=True).values[0:-1:2]\n",
    "            time_shallowest = subsampled_data.time.where(subsampled_data.dep == subsampled_data.dep.max(), drop=True).values[0:-1:2]\n",
    "            times = np.sort(np.concatenate((time_shallowest, time_deepest)))\n",
    "            # this results in a time grid that may not be uniformly spaced, but is correct\n",
    "            # - for a uniform grid, use the mean time spacing - may not be perfectly accurate, but is evenly spaced\n",
    "            dt = np.mean(np.diff(time_shallowest))/2 # average spacing of profiles (half of one up/down, so divide by two)\n",
    "            times_uniform = np.arange(survey_track.n_profiles.values*2) * dt\n",
    "\n",
    "        # nt is the number of profiles (times):\n",
    "        nt = len(times)  \n",
    "        # xgr is the vertical grid; nz is the number of depths for each profile\n",
    "        # depths are negative, so sort in reverse order using flip\n",
    "        zgridded = np.flip(np.unique(subsampled_data.dep.data))\n",
    "        nz = int(len(zgridded))\n",
    "\n",
    "        # -- initialize the dataset:\n",
    "        sgridded = xr.Dataset(\n",
    "            coords = dict(depth=([\"depth\"],zgridded),\n",
    "                      time=([\"time\"],times))\n",
    "        )\n",
    "        # -- 3-d fields: loop & reshape 3-d data from profiles to a 2-d (depth-time) grid:\n",
    "        # first, extract each variable, then reshape to a grid\n",
    "        \n",
    "        for vbl in vbls3d:\n",
    "            print(vbl)\n",
    "            if sampling_details['SAVE_PRELIMINARY']:\n",
    "                # not a dask array, so no \"compute\" command needed\n",
    "                this_var = subsampled_data[vbl].data.copy() \n",
    "            else:\n",
    "                this_var = subsampled_data[vbl].data.compute().copy() \n",
    "            # reshape to nz,nt\n",
    "            this_var_reshape = np.reshape(this_var,(nz,nt), order='F') # fortran order is important!\n",
    "            # for platforms with up & down profiles (uCTD and glider),\n",
    "            # every second column is upside-down (upcast data)\n",
    "            # starting with the first column, flip the data upside down so that upcasts go from top to bottom\n",
    "            if SAMPLING_STRATEGY != 'sim_mooring':\n",
    "                this_var_fix = this_var_reshape.copy()\n",
    "                #this_var_fix[:,0::2] = this_var_fix[-1::-1,0::2] \n",
    "                this_var_fix[:,1::2] = this_var_fix[-1::-1,1::2]  # Starting with SECOND column\n",
    "                sgridded[vbl] = ((\"depth\",\"time\"), this_var_fix)\n",
    "            elif SAMPLING_STRATEGY == 'sim_mooring':\n",
    "                sgridded[vbl] = ((\"depth\",\"time\"), this_var_reshape)\n",
    "                \n",
    "                \n",
    "        if sampling_details['DERIVED_VARIABLES']:\n",
    "            # for sampled steric height, we want the value integrated from the deepest sampling depth:\n",
    "            sgridded['steric_height'] = ((\"time\"), sgridded['steric_height'].isel(depth=nz-1).data)\n",
    "            # rename to \"steric_height_sampled\" for clarity\n",
    "            sgridded.rename_vars({'steric_height':'steric_height_sampled'})\n",
    "\n",
    "  \n",
    "\n",
    "        #  -- 2-d fields: loop & reshape 2-d data to the same time grid \n",
    "        for vbl in vbls2d:\n",
    "            \n",
    "            if sampling_details['SAVE_PRELIMINARY']:\n",
    "                # not a dask array, so no \"compute\" command needed\n",
    "                this_var = subsampled_data[vbl].data.copy() \n",
    "            else:\n",
    "                this_var = subsampled_data[vbl].data.compute().copy() \n",
    "            # subsample to nt\n",
    "            this_var_sub = this_var[0:-1:nz]\n",
    "            sgridded[vbl] = ((\"time\"), this_var_sub)\n",
    "\n",
    "    # ------------ RETURN INTERPOLATED & GRIDDED DATA ------------\n",
    "\n",
    "    # -- add variable attributes from ds\n",
    "    if SAMPLING_STRATEGY == 'mooring':\n",
    "        sgridded.attrs = ds.attrs\n",
    "    else:\n",
    "        # - find which variables in ds are also in our interpolated dataset:\n",
    "        vars_ds = list(ds.keys())\n",
    "        vars_sdata = list(subsampled_data.keys())\n",
    "        vars_both = list(set(vars_ds) & set(vars_sdata))\n",
    "        for var in vars_both:\n",
    "            # copy over the attribute from ds:\n",
    "            subsampled_data[var].attrs = ds[var].attrs\n",
    "            sgridded[var].attrs = ds[var].attrs\n",
    "    \n",
    "    \n",
    "    \n",
    "    return subsampled_data, sgridded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b557e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# great circle distance (from Jake Steinberg) \n",
    "def great_circle(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    return 6371 * (acos(sin(lat1) * sin(lat2) + cos(lat1) * cos(lat2) * cos(lon1 - lon2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8496313a",
   "metadata": {},
   "source": [
    "## Checks and Working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c139b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# USER INPUTS:\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# specify region from this list:\n",
    "# WesternMed  ROAM_MIZ  NewCaledonia  NWPacific  BassStrait  RockallTrough  ACC_SMST\n",
    "# MarmaraSea  LabradorSea  CapeBasin\n",
    "RegionName = 'ACC_SMST' \n",
    "\n",
    "# specify date range as start date & number of days.\n",
    "start_date = date(2012,1,1)\n",
    "# NOTE: ndays must be >1 \n",
    "ndays = 10\n",
    "\n",
    "# directory where data files are stored\n",
    "\n",
    "# kd: lab\n",
    "#datadir = '/Users/kdrushka/data/adac/mitgcm/netcdf/' + RegionName + '/'                      # input model data are here\n",
    "#outputdir = '/Users/kdrushka/data/adac/osse_output/' + RegionName + '/'           # interpolated data stored here\n",
    "#figdir = '/Users/kdrushka/Dropbox/projects/adac/figures/' + RegionName + '/' # store figures\n",
    "\n",
    "#mb: laptop\n",
    "#datadir = '/Volumes/TOSHIBA EXT 1/LLC4320_pre-SWOT_10_days/ACC_SMST/osse_model_input'  # input model data are here\n",
    "#outputdir = '/Volumes/TOSHIBA EXT 1/LLC4320_pre-SWOT_10_days/ACC_SMST/osse_output'   # interpolated data stored here\n",
    "#figdir = '/Volumes/TOSHIBA EXT 1/LLC4320_pre-SWOT_10_days/ACC_SMST/figures' # store figures\n",
    "\n",
    "#ocean computer\n",
    "#datadir = '/mnt/data/CapeBasin/osse_model_input/30days'  # input model data are here\n",
    "#outputdir = '/mnt/data/CapeBasin/osse_output'   # interpolated data stored here\n",
    "#figdir = '/mnt/data/CapeBasin/figures' # store figures#datadir = '/home/manjaree/Documents/LLC4320_pre-SWOT_10_days/ACC_SMST/osse_model_input/'  # input model data are here\n",
    "\n",
    "#ocean computer\n",
    "datadir = '/mnt/data/ACC_SMST/osse_model_input/10days'  # input model data are here\n",
    "outputdir = '/mnt/data/ACC_SMST/osse_output'   # interpolated data stored here\n",
    "figdir = '/mnt/data/ACC_SMST/figures' # store figures#datadir = '/home/manjaree/Documents/LLC4320_pre-SWOT_10_days/ACC_SMST/osse_model_input/'  # input model data are here\n",
    "\n",
    " \n",
    " #datadir = '/mnt/data/ACC_SMST/osse_model_input/30days'  # input model data are here\n",
    "#outputdir = '/mnt/data/ACC_SMST/osse_output'   # interpolated data stored here\n",
    "#figdir = '/mnt/data/ACC_SMST/figures' # store figures#datadir = '/home/manjaree/Documents/LLC4320_pre-SWOT_10_days/ACC_SMST/osse_model_input/'  # input model data are here\n",
    "\n",
    "#outputdir = '/home/manjaree/Documents/LLC4320_pre-SWOT_10_days/ACC_SMST/osse_output'   # interpolated data stored here\n",
    "#figdir = '/home/manjaree/Documents/LLC4320_pre-SWOT_10_days/ACC_SMST/figures' # store figures\n",
    "\n",
    "SAVE_FIGURES = True # True or False\n",
    "\n",
    "\n",
    "# optional details for sampling (if not specified, reasonable defaults will be used)\n",
    "# NOTE!! mooring and sim_mooring are different:\n",
    "#    sim_mooring treats the mooring datapoints like a glider, \n",
    "#    whereas mooring interpolates directly to the mooring grid and should be faster\n",
    "sampling_details_sim_glider = {\n",
    "  'SAMPLING_STRATEGY' : 'sim_glider', \n",
    "#   'SAMPLING_STRATEGY' : 'trajectory_file', # options: sim_glider, sim_uctd, wave_glider or trajectory_file.add:  ASV\n",
    "#   'SAMPLING_STRATEGY' : 'mooring', # options: sim_glider, sim_uctd, sim_mooring or trajectory_file.add: ASV. \n",
    "#    'SAMPLING_STRATEGY' : 'wave_glider', # options: wave_glider ..add trajectory file too?\n",
    "    'PATTERN' : 'lawnmower', # back-forth or lawnmower \n",
    "   'zrange' : [-1, -1000],  # depth range of T/S profiles (down is negative). * add U/V range? *\n",
    "#   'zmooring_TS' : list(range(-10,-1000,-10)) # instrument depths for moorings. T/S and U/V are the same.\n",
    " #   'zrange' : [-6, -100], # instrument depths for wave glider.  \n",
    "  'hspeed' : 0.25,  # platform horizontal speed in m/s (for glider, uCTD)\n",
    " #   'hspeed' : 1,# platform horizontal (profile) speed in m/s  (for wave_glider)\n",
    "   'vspeed' : 0.1, # platform vertical (profile) speed in m/s  (for glider, uCTD)\n",
    "  # 'vspeed': 0 ,# platform vertical (profile) speed in m/s  (for wave_glider)\n",
    "    'trajectory_file' : '../data/survey_trajectory_ACC_SMST_glider.nc', # if SAMPLING_STRATEGY = 'trajectory_file', specify trajectory file\n",
    "    'AT_END' : 'reverse', # behaviour at and of trajectory: 'reverse', 'repeat' or 'terminate'. (could also 'restart'?)\n",
    "    'DERIVED_VARIABLES' : False, # specify whether or not to process the derived variables (steric height, rotated velocity, vorticity) - slower and takes significant to derive/save the stored variables\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b228b6e-3bea-4715-a958-563d378b1b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_details_mooring = {\n",
    "#  'SAMPLING_STRATEGY' : 'sim_glider', \n",
    "#   'SAMPLING_STRATEGY' : 'trajectory_file', # options: sim_glider, sim_uctd, wave_glider or trajectory_file.add:  ASV\n",
    "   'SAMPLING_STRATEGY' : 'mooring', # options: sim_glider, sim_uctd, sim_mooring or trajectory_file.add: ASV. \n",
    "#    'SAMPLING_STRATEGY' : 'wave_glider', # options: wave_glider ..add trajectory file too?\n",
    "    'PATTERN' : 'lawnmower', # back-forth or lawnmower \n",
    "#   'zrange' : [-1, -1000],  # depth range of T/S profiles (down is negative). * add U/V range? *\n",
    "  'zmooring_TS' : list(range(-10,-1000,-10)), # instrument depths for moorings. T/S and U/V are the same.\n",
    " #   'zrange' : [-6, -100], # instrument depths for wave glider.  \n",
    " # 'hspeed' : 0.25,  # platform horizontal speed in m/s (for glider, uCTD)\n",
    " #   'hspeed' : 1,# platform horizontal (profile) speed in m/s  (for wave_glider)\n",
    "#   'vspeed' : 0.1, # platform vertical (profile) speed in m/s  (for glider, uCTD)\n",
    "  # 'vspeed': 0 ,# platform vertical (profile) speed in m/s  (for wave_glider)\n",
    "    'trajectory_file' : '../data/survey_trajectory_ACC_SMST_glider.nc', # if SAMPLING_STRATEGY = 'trajectory_file', specify trajectory file\n",
    "#    'AT_END' : 'reverse', # behaviour at and of trajectory: 'reverse', 'repeat' or 'terminate'. (could also 'restart'?)\n",
    "    'DERIVED_VARIABLES' : False, # specify whether or not to process the derived variables (steric height, rotated velocity, vorticity) - slower and takes significant to derive/save the stored variables\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242f06f6-ad0c-421c-8b19-382b0fab4116",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling_details_mooring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f8813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TBR\n",
    "datadir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7246918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TBR\n",
    "print(os.listdir(datadir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e73eeec-f137-4564-9352-748ad6b95f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TBR\n",
    "print(os.listdir(outputdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cbf0d4-4c0b-4d6e-9587-e32aefe37d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TBR\n",
    "print(os.listdir(figdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ac0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TBR\n",
    "#preview sampling details\n",
    "print(sampling_details_sim_glider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e4680f",
   "metadata": {},
   "source": [
    "# Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ac86ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MB\n",
    "sampling_details_wave_glider = {\n",
    "  'SAMPLING_STRATEGY' : 'sim_glider', \n",
    "#   'SAMPLING_STRATEGY' : 'trajectory_file', # options: sim_glider, sim_uctd, wave_glider or trajectory_file.add:  ASV\n",
    "#   'SAMPLING_STRATEGY' : 'mooring', # options: sim_glider, sim_uctd, sim_mooring or trajectory_file.add: ASV. \n",
    " #   'SAMPLING_STRATEGY' : 'wave_glider', # options: wave_glider ..add trajectory file too?\n",
    "    'PATTERN' : 'back-forth', # back-forth or lawnmower \n",
    " #  'zrange' : [-1, -1000],  # depth range of T/S profiles (down is negative). * add U/V range? *\n",
    "#   'zmooring_TS' : list(range(-10,-1000,-10)) # instrument depths for moorings. T/S and U/V are the same.\n",
    " #   'zrange' : [-6, -100], # instrument depths for wave glider.  \n",
    " # 'hspeed' : 0.25,  # platform horizontal speed in m/s (for glider, uCTD)\n",
    " #   'hspeed' : 1,# platform horizontal (profile) speed in m/s  (for wave_glider)\n",
    "  # 'vspeed' : 0.1, # platform vertical (profile) speed in m/s  (for glider, uCTD)\n",
    " #  'vspeed': 0.0001 ,# platform vertical (profile) speed in m/s  (for wave_glider)\n",
    "    'trajectory_file' : '../data/survey_trajectory_ACC_SMST_glider.nc', # if SAMPLING_STRATEGY = 'trajectory_file', specify trajectory file\n",
    "  #  'AT_END' : 'terminate', # behaviour at and of trajectory: 'reverse', 'repeat' or 'terminate'. (could also 'restart'?)\n",
    "    'DERIVED_VARIABLES' : False # specify whether or not to process the derived variables (steric height, rotated velocity, vorticity) - slower and takes significant to derive/save the stored variables\n",
    "}\n",
    "#MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bac45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling_details = sampling_details_mooring\n",
    "sampling_details = sampling_details_sim_glider\n",
    "#sampling_details = sampling_details_wave_glider\n",
    "sampling_details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413149bf",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9550a175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following two lines if troubleshooting osse_tools\n",
    "# del sys.modules['osse_tools']  \n",
    "# from osse_tools import download_llc4320_data, compute_derived_fields, get_survey_track, survey_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964c682f-4b28-463e-abcc-828fefc7305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# download files:\n",
    "#download_llc4320_data(RegionName, datadir, start_date, ndays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e7f82f-5010-4733-9526-a1e38291889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7265f7ea-a7b7-4e4a-b977-bc7ac89dfca6",
   "metadata": {},
   "source": [
    "# Trying without compute derived fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d8b311-6fc4-4b61-b584-4be3aa45af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive & save new files with steric height & vorticity\n",
    "#if sampling_details['DERIVED_VARIABLES']:\n",
    " #  compute_derived_fields1(RegionName, datadir, start_date, ndays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e29210-f2a6-4cc3-a4ef-af5674851d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37f08ff-553e-4257-b01d-d1b45a1d4173",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da66f91-f7ed-4ddb-bc53-7434a20409ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "date_list = [start_date + timedelta(days=x) for x in range(ndays)]\n",
    "#target_files = [f'{datadir}_{date_list[n].strftime(\"%Y%m%d\")}.nc' for n in range(ndays)] # list target files\n",
    "target_files = [f'{datadir}/LLC4320_pre-SWOT_ACC_SMST_{date_list[n].strftime(\"%Y%m%d\")}.nc' for n in range(ndays)] # lis\n",
    "target_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4b2a21",
   "metadata": {},
   "source": [
    "#### Load all model data files. \n",
    "If `DERIVED_VARIABLES = True`, do *not* load U, V, or oceTAUX, oceTAUY as we will replace these with transformed versions .\n",
    "\n",
    "Note that if `DERIVED_VARIABLES = False`, the vector variables U,V, TAUX and TAUY are in the reference frame of the model and must be rotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7126bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ndays=30\n",
    "date_list = [start_date + timedelta(days=x) for x in range(ndays)]\n",
    "#target_files = [f'{datadir}_{date_list[n].strftime(\"%Y%m%d\")}.nc' for n in range(ndays)] # list target files\n",
    "target_files = [f'{datadir}/LLC4320_pre-SWOT_ACC_SMST_{date_list[n].strftime(\"%Y%m%d\")}.nc' for n in range(ndays)] # lis\n",
    "# chunk size ... aiming for ~100 MB chunks\n",
    "# these chunks seem to work OK for up to ~20 day simulations, but more \n",
    "# testing is needed to figure out optimal parameters for longer simulations\n",
    "#tchunk = 6 \n",
    "#xchunk = 200\n",
    "#ychunk = 200\n",
    "\n",
    "#original\n",
    "#tchunk = 6 \n",
    "#xchunk = 150\n",
    "#ychunk = 150\n",
    "\n",
    "# drop the vector variables if loading derived variables because we are going to load the rotated ones in the next cell\n",
    "#if sampling_details['DERIVED_VARIABLES']:\n",
    " #   drop_variables={'U', 'V', 'oceTAUX', 'oceTAUY'}\n",
    "#else:\n",
    " #   drop_variables={}\n",
    "\n",
    "ds = xr.open_mfdataset(target_files, \n",
    "                       #parallel=True, \n",
    "                       #drop_variables=drop_variables,\n",
    "                      #chunks={'i':xchunk, 'j':ychunk, 'time':tchunk}\n",
    "                      )\n",
    "\n",
    "# XC, YC and Z are the same at all times, so select a single time\n",
    "# (note, this breaks for a single file - always load >1 file)\n",
    "X = ds.XC.isel(time=0) \n",
    "Y = ds.YC.isel(time=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38af3d1-87c4-45ca-af32-78bd0fa703b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502768d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# load the corresponding derived fields (includes steric height, vorticity, and transformed vector variables for current and wind stress)\n",
    "if sampling_details['DERIVED_VARIABLES']:\n",
    "    derivedir = datadir + 'derived/'\n",
    "    derived_files = [f'{derivedir}LLC4320_pre-SWOT_{RegionName}_derived-fields_{date_list[n].strftime(\"%Y%m%d\")}.nc' for n in range(ndays)] # list target files\n",
    "    dsd = xr.open_mfdataset(derived_files, parallel=True, chunks={'i':xchunk, 'j':ychunk, 'time':tchunk})\n",
    "    \n",
    "    # merge the derived and raw data\n",
    "    ds = ds.merge(dsd)\n",
    "    # rename the transformed vector variables to their original names\n",
    "    ds = ds.rename_vars({'U_transformed':'U', 'V_transformed':'V', \n",
    "                         'oceTAUX_transformed':'oceTAUX', 'oceTAUY_transformed':'oceTAUY'})\n",
    "\n",
    "\n",
    "# drop a bunch of other vars we don't actually use - can comment this out if these are wanted\n",
    "ds = ds.drop_vars({'DXV','DYU', 'DXC','DXG', 'DYC','DYG', 'XC_bnds', 'YC_bnds', 'Zp1', 'Zu','Zl','Z_bnds', 'nb'})\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08bb650",
   "metadata": {},
   "source": [
    "### Create & plot sampling track\n",
    "Based on the parameters in `sampling_details`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3744e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#del sys.modules['osse_tools_Copy1'] \n",
    "#from osse_tools_Copy1 import download_llc4320_data, compute_derived_fields, get_survey_track, survey_interp\n",
    "\n",
    "survey_track, survey_indices, sampling_parameters = get_survey_track(ds, sampling_details)\n",
    "\n",
    "# print specified sampling_details + any default values\n",
    "print(sampling_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831170f0-e2e3-4398-b12f-3d08d41e3f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling_parameters['hspeed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b615440",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527fac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe2388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- generate name of file to save outputs in ---- \n",
    "filename_base = (f'OSSE_{RegionName}_{sampling_details[\"SAMPLING_STRATEGY\"]}_{start_date}_to_{start_date + timedelta(ndays)}_maxdepth{int(sampling_parameters[\"zrange\"][1])}')\n",
    "filename_out_base = (f'{outputdir}/{filename_base}')\n",
    "print(filename_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9e05bf-e466-44a5-9be6-0c867943d278",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filename_out_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbc6df0",
   "metadata": {},
   "source": [
    "### Visualize the track over a single model snapshot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575419ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "# map of Theta at time zero\n",
    "ax = plt.subplot(1,2,1)\n",
    "ssto = plt.pcolormesh(X,Y,ds.Theta.isel(k=0, time=0).values, shading='auto')\n",
    "if not (sampling_details['SAMPLING_STRATEGY'] == 'mooring' or sampling_details['SAMPLING_STRATEGY'] == 'sim_mooring'):\n",
    "    tracko = plt.scatter(survey_track.lon, survey_track.lat, c=(survey_track.time-survey_track.time[0])/1e9/86400, cmap='Reds', s=0.75)\n",
    "    plt.colorbar(ssto).set_label('SST, $^o$C')\n",
    "    plt.colorbar(tracko).set_label('days from start')\n",
    "    plt.title('SST and survey track: ' + RegionName + ', '+ sampling_details['SAMPLING_STRATEGY'])\n",
    "else:\n",
    "    plt.plot(survey_track.lon, survey_track.lat, marker='*', c='r')\n",
    "    plt.title('SST and mooring location: ' + RegionName + ' region, ' + sampling_details['SAMPLING_STRATEGY'] )\n",
    "\n",
    "\n",
    "# depth/time plot of first few datapoints\n",
    "ax = plt.subplot(1,2,2)\n",
    "iplot = slice(0,20000)\n",
    "if not (sampling_details['SAMPLING_STRATEGY'] == 'mooring' or sampling_details['SAMPLING_STRATEGY'] == 'sim_mooring'):\n",
    "    plt.plot(survey_track.time.isel(points=iplot), survey_track.dep.isel(points=iplot), marker='.')\n",
    "else:\n",
    "    # not quite right but good enough for now.\n",
    "    # (times shouldn't increase with depth)\n",
    "    plt.scatter((np.tile(survey_track['time'].isel(time=iplot), int(survey_track['dep'].data.size))),\n",
    "         np.tile(survey_track['dep'], int(survey_track['time'].isel(time=iplot).data.size)),marker='.')             \n",
    "#plt.xlim([start_date + datetime.timedelta(days=0), start_date + datetime.timedelta(days=2)])\n",
    "plt.ylabel('Depth, m')\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.title(f\"Sampling pattern, hspeed ={sampling_parameters['hspeed']}, vspeed ={sampling_parameters['vspeed']}\")\n",
    "\n",
    "\n",
    "# save\n",
    "if SAVE_FIGURES:\n",
    "    #plt.savefig('/data2/Dropbox/projects/adac/figures/' + filename_base + '_sampling.png', dpi=400, transparent=False, facecolor='white')\n",
    "    plt.savefig(figdir + '/' + filename_base + '_sampling.png', dpi=400, transparent=False, facecolor='white')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569d0e51-a7a1-4c0c-840f-a01fba928fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "figdir + '/' + filename_base + '_sampling.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385ad9b2-c1b4-4238-a19f-e1a116f057a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(survey_track.time-survey_track.time[0])/1e9/86400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8624f4a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampling_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4823c938",
   "metadata": {},
   "source": [
    "### Interpolate data with the specified sampling pattern (this is where the magic happens!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d605f821",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#del sys.modules['osse_tools'] \n",
    "#from osse_tools import survey_interp, get_survey_track\n",
    "\n",
    "subsampled_data, sgridded = survey_interp(ds, survey_track, survey_indices, sampling_parameters)\n",
    "sgridded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4333a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 3d fields\n",
    "vbls3d = ['Theta','Salt','U','V','vorticity']\n",
    "vbls3d = ['Theta','Salt']\n",
    "ylim = [min(sgridded['depth'].values), max(sgridded['depth'].values)]\n",
    "ylim = [-200, -1]\n",
    "\n",
    "nr = len(vbls3d) # # of rows\n",
    "fig,ax=plt.subplots(nr,figsize=(8,len(vbls3d)*2),constrained_layout=True)\n",
    "\n",
    "\n",
    "for j in range(nr):\n",
    "    sgridded[vbls3d[j]].plot(ax=ax[j], ylim=ylim)\n",
    "    ax[j].plot(sgridded.time.data, -sgridded.KPPhbl.data, c='k')\n",
    "    ax[j].set_title(vbls3d[j])\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(figdir + '/' + filename_base + '_3D.png', dpi=400, transparent=False, facecolor='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93360c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fb48ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## selected 2d fields\n",
    "j=0\n",
    "nr = 6 # # of rows\n",
    "fig,ax=plt.subplots(nr,figsize=(10,8),constrained_layout=True)\n",
    "\n",
    "\n",
    "# wind vectors\n",
    "ax[j].quiver(sgridded.time.data,0,sgridded.oceTAUX.data, sgridded.oceTAUY.data)\n",
    "ax[j].set_title('Wind stress')    \n",
    "ax[j].set_ylabel('N m-2')\n",
    "# SH \n",
    "j+=1\n",
    "ax[j].plot(sgridded.time,sgridded.steric_height-sgridded.steric_height.mean(), \n",
    "             sgridded.time.data,sgridded.steric_height_true-sgridded.steric_height_true.mean())\n",
    "ax[j].set_title('Steric height')\n",
    "ax[j].legend(['subsampled','true'])\n",
    "ax[j].set_ylabel('m')\n",
    "\n",
    "# SSH\n",
    "j+=1\n",
    "ax[j].plot(sgridded.time,sgridded.Eta)\n",
    "ax[j].set_title('SSH')\n",
    "ax[j].set_ylabel('m')\n",
    "\n",
    "# MLD\n",
    "j+=1\n",
    "ax[j].plot(sgridded.time,sgridded.KPPhbl)\n",
    "ax[j].set_title('MLD')\n",
    "ax[j].set_ylabel('m')\n",
    "ax[j].invert_yaxis()\n",
    "\n",
    "# surface heat flux\n",
    "j+=1\n",
    "ax[j].plot(sgridded.time,sgridded.oceQnet, sgridded.time,sgridded.oceQsw)\n",
    "ax[j].set_title('Surface heat flux into the ocean')\n",
    "ax[j].legend(['total','shortwave'])\n",
    "ax[j].set_ylabel('W m-2')\n",
    "\n",
    "# surface FW flux\n",
    "j+=1\n",
    "ax[j].plot(sgridded.time,sgridded.oceFWflx)\n",
    "ax[j].set_title('Surface freshwater flux into the ocean') \n",
    "ax[j].set_ylabel('kg m-2 s-1')\n",
    "\n",
    "# horiz line:\n",
    "for j in range(nr):\n",
    "    ax[j].axhline(0, color='grey', linewidth=0.8)\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(figdir + '/' + filename_base + '_2D.png', dpi=400, transparent=False, facecolor='white')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67154dc3-4cdf-4485-a9a4-06266f996563",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
